{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4de39d1-34a5-4cf0-a6bb-74070db545f3",
   "metadata": {},
   "source": [
    "# PEFT Example (LoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a0472d68-c388-4c15-8920-68df4273e1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "# all of these are HuggingFace libraries:\n",
    "import datasets\n",
    "import peft\n",
    "import transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367c6813-c002-4726-bf62-ad5a2a015ff3",
   "metadata": {},
   "source": [
    "## Load Pre-Trained Model and Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df2388e-7c8c-4a96-9620-04a0f88f944c",
   "metadata": {},
   "source": [
    "Pick a pre-trained model from the [HuggingFace models hub](https://huggingface.co/models) for the type of task you're interested in. Here I choose a GPT2 variant for causal language modelling (text generation). Each model has an associated tokenizer to preprocess text into the correct format (e.g. correct token ids) for that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9cc5780f-adac-482a-9603-3d85d6d9d8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2-medium\"\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db640cfd-3fed-4e16-9940-1d1d9a7254ee",
   "metadata": {},
   "source": [
    "We can have a look at the architecture of the model and how many parameters it contains (you can see it has 24 `GPT2Block`s which each contain an attention module, and about 350 million parameters):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a1ecb4ab-8f22-4b67-b35f-cd7fa6575f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 1024)\n",
      "    (wpe): Embedding(1024, 1024)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-23): 24 x GPT2Block(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
      ")\n",
      "No. parameters:  354823168\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print(\"No. parameters: \", model.num_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282bc743-75cc-42b0-895f-0c180abf4c06",
   "metadata": {},
   "source": [
    "## Example Generations from Base Model\n",
    "\n",
    "Get the base model to generate some text with a blank input prompt, which gives an idea about the types of text/topics that appeared a lot in the pre-training dataset (seems to be a lot of mostly US news, politics, and entertainment). You can also try with different prompts if you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7d577d21-fdc9-41ac-9802-ad63b46157b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In his new book on economics, Adam Smith: A Modern History, Martin Feldmeier points to two main features that helped shape economics. To those who remember his arguments, they will feel comfortable and at ease. The first is Smith's reliance on \n",
      " ------\n",
      "The following pages provide examples of popular programs that can help you learn the programming language Ruby. Some of the best examples are listed in the Order the Programming Languages, but most tutorials here also demonstrate other languages; those examples are listed as reference, not \n",
      " ------\n",
      "Mitt Romney. NBC News Photo credit: Nick Ut\n",
      "\n",
      "Mitt Romney is one of the world's most outspoken and well known feminists ‚Äî but the American right wing is still waiting for what he'd call a \"feminist\" candidate. \n",
      " ------\n"
     ]
    }
   ],
   "source": [
    "def print_samples(model, tokenizer, prompt=\"\", n_samples=3):\n",
    "    pipe = transformers.pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=\"mps\")\n",
    "    n_samples = 3\n",
    "    samples = pipe(prompt, num_return_sequences=3)\n",
    "    for s in samples:\n",
    "        print(s[\"generated_text\"], \"\\n ------\")\n",
    "\n",
    "\n",
    "prompt = \"\"\n",
    "print_samples(model, tokenizer, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c7270d-14b0-4657-93e7-306ab7b89631",
   "metadata": {},
   "source": [
    "## Load a Dataset of a Celebrity's Tweets\n",
    "\n",
    "I've downloaded datasets of tweets from a few different celebrities [from Kaggle](https://www.kaggle.com/datasets/ahmedshahriarsakib/top-1000-twitter-celebrity-tweets-embeddings). Here are a few from the actor Anna Kendrick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c13ca40-d1ef-4053-aaed-cadf07f09b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@RyBrockington granddaughter of Max \n",
      "\n",
      "Go Pats!!!! \n",
      "\n",
      "Pinky, are you pondering what I'm pondering? \n",
      "\n",
      "My day job. http://t.co/zbz6r3SKEr \n",
      "\n",
      "My first time in sales‚Ä¶ not sure if I'm doing this right‚Ä¶ https://t.co/ZFAkKyIfr1 https://t.co/LEq9a5H2YJ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "celebrity = \"annakendrick\"\n",
    "df = pandas.read_csv(f\"{celebrity}.csv\")\n",
    "for t in df[\"tweet\"].sample(5):\n",
    "    print(t, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881cfeb7-0945-40ed-a732-5aacdb7eeffd",
   "metadata": {},
   "source": [
    "Convert the dataframe into a HuggingFace dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b613eb1a-b308-4b05-a974-e618a7d9afc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tweet'],\n",
       "    num_rows: 2468\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.Dataset.from_pandas(df)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be4d5d0-2782-4683-890b-ae199b262080",
   "metadata": {},
   "source": [
    "And preprocess the dataset by running it through the tokenizer (removing the original `tweet` feature afterwards as the training will use the created `input_ids` and `attention_mask` features instead):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "452353e5-d245-4e1e-aadc-7abcbf69fd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9835f28f335a41558702c495e6aa4826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2468 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 2468\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.map(lambda sample: tokenizer(sample[\"tweet\"]), remove_columns=dataset.features)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb999cd-d7e1-4bf5-a86f-5390627bd910",
   "metadata": {},
   "source": [
    "## Create a PEFT LoRA Config\n",
    "\n",
    "To prepare a model for PEFT fine-tuning you need to create a config for the PEFT method you're using, in this case `LoraConfig`, and then call `peft.get_peft_model` with the base model and PEFT config. Here's a default LoRA setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "04b5e0a0-ef45-4ab7-a8e8-213602c6edbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jroberts/repos/transformers-reading-group/sessions/11-peft/.venv/lib/python3.11/site-packages/peft/tuners/lora.py:475: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "peft_config = peft.LoraConfig(\n",
    "    task_type=peft.TaskType.CAUSAL_LM,\n",
    "    r = 8,  # LoRA \"rank\" (small dimension of learnt A, B matrices)\n",
    "    target_modules=None,  # which layers to apply LoRA to (has model-specific defaults, usually attention queries and values)\n",
    ")\n",
    "peft_model = peft.get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42a96e2-6b39-4d6d-ad61-5621a524fa90",
   "metadata": {},
   "source": [
    "The adapted PEFT model contains additional LoRA modules, for example the (low rank) matrices `A` and `B` which are used to learn the adjustment to the attention weights (you can see that `out_features` of `A` and `in_features` of `B` are both 8 - the LoRA rank specified in the config).\n",
    "\n",
    "In this case LoRA will fine-tune about 0.2% of the total parameters in the model (fewer than a million out of 350 million parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "15fdbcba-31d8-4276-b9ed-44e527548835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): GPT2LMHeadModel(\n",
      "      (transformer): GPT2Model(\n",
      "        (wte): Embedding(50257, 1024)\n",
      "        (wpe): Embedding(1024, 1024)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "        (h): ModuleList(\n",
      "          (0-23): 24 x GPT2Block(\n",
      "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPT2Attention(\n",
      "              (c_attn): Linear(\n",
      "                in_features=1024, out_features=3072, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (c_proj): Conv1D()\n",
      "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPT2MLP(\n",
      "              (c_fc): Conv1D()\n",
      "              (c_proj): Conv1D()\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "trainable params: 786,432 || all params: 355,609,600 || trainable%: 0.2211503851414585\n"
     ]
    }
   ],
   "source": [
    "print(peft_model)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b33419-9df7-4010-8add-95aa49a2a4e9",
   "metadata": {},
   "source": [
    "## Fine-Tune the PEFT model on the tweets\n",
    "\n",
    "Training the PEFT variant of the model now should just work the same as training any HuggingFace model with the HuggingFace `Trainer` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "109ff829-3026-4a06-860a-e705c6d42634",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = transformers.TrainingArguments(\n",
    "    \"tmp\",\n",
    "    num_train_epochs=0.1, # increase num_train_epochs to at least a few whole epochs if you want to see a clear difference in the fine-tuned model\n",
    "    save_strategy=\"no\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "142c82c6-2762-4f81-9e11-8ee6a4c2c438",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    peft_model,\n",
    "    args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d2e47a2-a05d-407d-9e27-6788900a2c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31' max='31' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [31/31 00:22, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=31, training_loss=2.728060814642137, metrics={'train_runtime': 23.1772, 'train_samples_per_second': 10.648, 'train_steps_per_second': 1.338, 'total_flos': 22492285698048.0, 'train_loss': 2.728060814642137, 'epoch': 0.1})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15eac083-f047-46b4-8b3c-f68eb46631af",
   "metadata": {},
   "source": [
    "## Save the LoRA weights\n",
    "\n",
    "When saving the model only the LoRA weights are saved, which are a few megabytes (see the size of `adapter_model.bin` below) rather than the 1.5 GB of the original model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1607e542-c45c-4edb-b25a-012e1af5f158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "total 6200\n",
      "-rw-r--r--@ 1 jroberts  staff    88B Sep 25 14:49 README.md\n",
      "-rw-r--r--@ 1 jroberts  staff   416B Sep 25 14:49 adapter_config.json\n",
      "-rw-r--r--@ 1 jroberts  staff   3.0M Sep 25 14:49 adapter_model.bin\n"
     ]
    }
   ],
   "source": [
    "peft_model.save_pretrained(f\"{celebrity}\")\n",
    "\n",
    "!ls -lh {celebrity}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a79fe7-8dc9-4e6f-a727-df18c446e1d4",
   "metadata": {},
   "source": [
    "## Load the fine-tuned model\n",
    "\n",
    "`AutoModelForCausalLM.from_pretrained` does two steps in the background:\n",
    "\n",
    "- Load the appropriate base model (defined in the saved PEFT config)\n",
    "- Load the LoRA weights to adapt the base model\n",
    "\n",
    "At this point, you could also choose to merge the LoRA weights with the base weights (do the additions $W_0 + AB$), in which case there would be no additional latency cost to using a LoRA-adjusted model (see the [PEFT documentation](https://huggingface.co/docs/peft/index) for how to do this).\n",
    "\n",
    "Generating some more samples with a blank input prompt, you can see that our Anna Kendrick tweet fine-tuned model is more likely to talk about films, TV, or general day to day things than the original (news heavy) base model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01c90b46-ba54-42a7-bcf5-2511dc624394",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will Smith in 'Life of Pi'? It's a film no one seems to remember! Read on for the details. Read my other stories from this crazy first weekend: 1. http://twothirdsandthings.com/2014/08 \n",
      " ------\n",
      "A former executive with Twitter has spoken out against the company's latest decision to lock down data belonging to Twitter, arguing that it has given users the false impression that the company is controlling their data and that they are being sold data \"in return\" \n",
      " ------\n",
      "The only way to tell people I'm making some bullshit joke is to tell other people my fucking joke. I know, I know. I have it. No excuses. No apologies. I'm done. So let's move on. #Me \n",
      " ------\n"
     ]
    }
   ],
   "source": [
    "loaded_peft_model = peft.AutoPeftModelForCausalLM.from_pretrained(celebrity)\n",
    "print_samples(loaded_peft_model, tokenizer, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6504ab-77ef-47e0-9fe5-c9c9313c0e38",
   "metadata": {},
   "source": [
    "##¬†Other Celebrities\n",
    "\n",
    "In the repo there are LoRA adapters (trained on 10 epochs) for 5 celebrity tweets. Compare the differences in the type of text each model generates below. Note that the way I have implemented the loop below means the base model is re-loaded each time, you could swap in/out only the LoRA weights each time instead if you needed the model loading to be faster in deployment (again see the PEFT documentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9486ad28-20d2-4fa2-8b4b-df7d03f3b86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ===== annakendrick =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don't get that crazy guy from last show on the island. That's exactly him. #mitchkrollmovies ‚Äî Alex Malarkey (@alexmmalarkey) February 18, 2013\n",
      "\n",
      "Just made the mistake of \n",
      " ------\n",
      "The new world, with its endless streams of money and abundance, is truly a joy. There is so much shit sitting in the trash that you have to watch out for to see it, right? It's a wonderful thing. And just before \n",
      " ------\n",
      "The latest episode of 'Shark Tank' featured a woman with tattoos that resembled 'Frozen' characters - so why not?\n",
      "\n",
      "I was shocked and happy to get a sneak peak @SharkTankSBS. This show is way \n",
      " ------\n",
      "\n",
      " ===== billgates =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@magnificentbio You are a wonderful leader in science, and you deserve a thank you. I‚Äôm sorry to see you go (but‚Äô‚Äôwe‚Äôre thrilled to have you here). You‚Äô \n",
      " ------\n",
      "The world is watching #TheGreatFireWith #Africa, and @movietaprov is working on ways to help as well: https://t.co/5iXdSQ9V0n https://t. \n",
      " ------\n",
      "How do we build great infrastructure? A combination of #MITxCoral and #MITxSweden. This conversation @CoralWatch has been really encouraging. https://t.co/4ZjUqNlX4v \n",
      " ------\n",
      "\n",
      " ===== eminem =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the first major interview since stepping down from his role at Comedy Central's The Daily Show in October, Will Ferrell gives Rolling Stone an inside view of the first season of Mad Max: Fury Road ‚Ä¶ and he's not impressed. Watch the \n",
      " ------\n",
      "Shakespearean music that makes you fall from your chair in pain... https://t.co/8uX7s6FpN1 http://t.co/nBhN6b7r1nk http:// \n",
      " ------\n",
      "Episode #12: T-Shirt Design and Merchandise.\n",
      "This week, I have reoccurring guest Matt Zoller Seitz, one of the co-lead hosts of the upcoming show @TheRebel. Join as I take \n",
      " ------\n",
      "\n",
      " ===== oprah =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@WMABCTV: @VernonWayland joins us today to share stories about his journey, from the very beginning of his #HairGate story, to the battle to have his @OWNTV interview air on the @OWN \n",
      " ------\n",
      "Sunday, October 17, 2016\n",
      "The Story\n",
      "With the season upon us, it is time to share our favorite \"Gosh. I wonder about #TheHinduTribes.\" Read on for @TheCaucasianNation's exclusive take \n",
      " ------\n",
      "Just hours after taking responsibility for the death of his sister, Donta'a Williams says there was no malicious intent and that her death was a tragic accident.\n",
      "\n",
      "On Saturday in Orlando, Florida police officers arrested 18-year-old \n",
      " ------\n",
      "\n",
      " ===== elonmusk =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heaven is very happy that you have a sweet baby.\n",
      "\n",
      "Heaven's Little‚Äôs üôÅ (@crispywaffles) is a very small baby girl so I know it really likes cake.\n",
      "\n",
      "Heaven \n",
      " ------\n",
      "We had a great opportunity last night to tour with a very popular show band as guests on our very first @washington @TheMuskBlog podcast. This time around I took my time to talk with each of them, talk about the show \n",
      " ------\n",
      "He has been calling for a few years now to have a ban on all weapons that can be modelled after human beings.\n",
      "\n",
      "If we want a more peaceful world, we should be stopping humans from starting guns. #FreePalestine https \n",
      " ------\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\n",
    "celebrities = [\"annakendrick\", \"billgates\", \"eminem\", \"oprah\", \"elonmusk\"]\n",
    "for c in celebrities:\n",
    "    print(\"\\n\", \"=\" * 5, c, \"=\" * 5)\n",
    "    loaded_peft_model = peft.AutoPeftModelForCausalLM.from_pretrained(c)\n",
    "    print_samples(loaded_peft_model, tokenizer, prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
