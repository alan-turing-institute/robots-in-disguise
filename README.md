# _Robots in Disguise_: Fundamental AI Reading Group

Public repo for The Alan Turing Institute's reading group on fundamental AI research.

If you're based at the Turing, follow `#robots-in-disguise` on the Turing Slack for the most recent updates.

**To see all the slides and reading materials for previous sessions, see the [archive](PREVIOUS.md).**

Note that this originated from the [Research Engineering Team](https://www.turing.ac.uk/research-engineering)'s reading group on Transformers.

## Overview

The group meets every <b>week on Mondays at 11-12</b>. Everyone is welcome to join! If you have any questions email [Ryan Chan](mailto:rchan@turing.ac.uk), [Fede Nanni](mailto:fnanni@turing.ac.uk) or [Giulia Occhini](go292@cam.ac.uk) and remember to go through our [Code of Conduct](CodeOfConduct.md) before joining.

Please **get in touch** if you would like to give a talk (either about your research or a topic you think is relevant to the reading group) add suggestions and emoji preferences to the [list of proposed topics](https://hackmd.io/4zHl_1G6Se-yumHTN48dqg?both) on HackMD!

## Upcoming Schedule

|Date | Topic | Room | Lead |
| --- | ----- | ---- | ---- |
| [09/12/24](#091224) | Scaling laws of neural networks | David Blackwell | [Edmund Dable-Heath](https://github.com/eddableheath) |
| [16/12/24](#161224) | Improving training with better learning rate and batch size: Linear scaling rule from random matrix theory | David Blackwell | [Chanju Park](https://www.turing.ac.uk/people/doctoral-students/chanju-park) |

# Material for sessions

## 09/12/24
### Scaling laws of neural networks

- [Scaling Laws for Neural Language Models](https://arxiv.org/pdf/2001.08361)
- [Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556)

