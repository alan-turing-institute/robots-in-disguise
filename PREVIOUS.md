## Previous sessions

This page shows a history of previous sessions in the reading group. 

|Date | Topic | Room | Lead |
| --- | ----- | ---- | ---- |
| [20/03/23](#200323) | Introduction to word embeddings and language modelling ([Slides](https://docs.google.com/presentation/d/1i56HKtjcdQFTxacxsjgya_giDx8Mv1xZn-IDNc_mK8I/edit?usp=sharing)) | David Blackwell | [Fede Nanni](https://github.com/fedenanni) |
| [03/04/23](#030423) | Deep Learning Basics ([Slides](https://github.com/alan-turing-institute/foundation-models-reading-group/blob/main/sessions/02-deep-learning/Robots_Neural_Networks_2023-04-03.pptx)) | David Blackwell | [Phil Swatton](https://github.com/philswatton), [Jack Roberts](https://github.com/jack89roberts) |
| [17/04/23](#170423) | Sequence-to-sequence models part I: RNNs/LSTMs ([Slides](https://github.com/alan-turing-institute/foundation-models-reading-group/blob/main/sessions/03-seq2seq-part-i/seq2seq_part1_hut23_robots_in_disguise.pdf)) | David Blackwell | [Ryan Chan](https://github.com/rchan26) |
| [03/05/23](#030523) | Sequence-to-sequence models part II: Encoder-decoder models ([Slides](https://github.com/alan-turing-institute/foundation-models-reading-group/blob/main/sessions/04-seq2seq-part-ii/seq2seq_part2_hut23_robots_in_disguise.pdf)) | David Blackwell | [Ryan Chan](https://github.com/rchan26) |
| 15/05/23 | Hands-on RNN/LSTM session ([Materials](https://github.com/phinate/jax-rnn))| David Blackwell | [Nathan Simpson](https://github.com/phinate), [Levan Bokeria](https://github.com/lbokeria), [David Llewellyn-Jones](https://github.com/llewelld) |
| [31/05/23](#310523) | Reginald overview & Attention and self-attention networks ([Notebook](https://github.com/alan-turing-institute/foundation-models-reading-group/tree/main/REGinalds/gpt2-demo)) | David Blackwell | [Evelina Gabasova](https://github.com/evelinag), [Martin Stoffel](https://github.com/mastoffel) |
| [26/06/23](#260623) | Attention (continued) ([Slides](https://github.com/alan-turing-institute/foundation-models-reading-group/blob/main/sessions/05-attention/attention.pdf)) & Transformer Encoder and Decoders ([Slides](https://github.com/alan-turing-institute/foundation-models-reading-group/blob/main/sessions/06-transformers-architecture/transformer_architecture_hut23_robots_in_disguise.pdf)) | David Blackwell | [Martin Stoffel](https://github.com/mastoffel), [Ryan Chan](https://github.com/rchan26) |
| [10/07/23](#100723) | BERT: Masked Language modelling and Pre-training ([Slides](https://github.com/alan-turing-institute/foundation-models-reading-group/blob/main/sessions/07-bert/bert_hut23_robots_in_disguise.pdf)) | David Blackwell | [Ryan Chan](https://github.com/rchan26) |
| [24/07/23](#240723) | GPT: Pretraining Decoders ([Slides](https://github.com/alan-turing-institute/foundation-models-reading-group/blob/main/sessions/08-gpt/gpt_hut23_robots_in_disguise.pdf)) | David Blackwell | [Ryan Chan](https://github.com/rchan26) |
| [07/08/23](#070823) | Vision Transformers part I ([Slides](https://github.com/alan-turing-institute/foundation-models-reading-group/blob/main/sessions/09-vision-transformers-part-i/vision_transformer_part1_hut23_robots_in_disguise.pdf)) | David Blackwell | [Katie Awty-Carroll](https://github.com/klh5), [Ryan Chan](https://github.com/rchan26) |
| [21/08/23](#210823) | Vision Transformers part II ([Slides](https://github.com/alan-turing-institute/foundation-models-reading-group/blob/main/sessions/10-vision-transformers-part-ii/vision_transformer_part2_hut23_robots_in_disguise.pdf)) | David Blackwell | [Katie Awty-Carroll](https://github.com/klh5) |
| [18/09/23](#180923) | LoRA (+ parameter efficient fine-tuning) part I ([Slides](https://github.com/alan-turing-institute/foundation-models-reading-group/blob/main/sessions/11-peft/20230915_PEFT.pdf)) | David Blackwell | [Jack Roberts](https://github.com/jack89roberts) |
| [25/09/23](#180923) | LoRA (+ parameter efficient fine-tuning) part II ([Notebook](https://github.com/alan-turing-institute/foundation-models-reading-group/blob/main/sessions/11-peft/peft.ipynb)) | Margaret Hamilton | [Jack Roberts](https://github.com/jack89roberts) |
| [02/10/23](#021023) | Reinforcement Learning Human Feedback (RLHF) ([Slides](https://github.com/alan-turing-institute/foundation-models-reading-group/blob/main/sessions/12-rlhf/2023-10-02-rlhf.pdf)) | David Blackwell | [Eseoghene Ben-Iwhiwhu](https://github.com/dlpbc) |
| [16/10/23](#161023) | Prompt Engineering ([Slides](https://github.com/alan-turing-institute/foundation-models-reading-group/blob/main/sessions/13-prompt-engineering/prompts.pdf)) | David Blackwell | [Martin Stoffel](https://github.com/mastoffel) |
| [30/10/23](#301023) | Technical: Knowledge retrieval ([Slides](https://github.com/alan-turing-institute/foundation-models-reading-group/blob/main/sessions/14-knowledge-retrieval-fms/Knowledge_Retrieval_FMs.pdf)) | David Blackwell | [Praveen Selvaraj](https://github.com/pravsels) |
| [06/11/23](#061123) | Discussion: Current challenges and future directions in safety evaluations for generative AI ([Slides](https://github.com/alan-turing-institute/foundation-models-reading-group/blob/main/sessions/15-safety-evaluations-for-generative-ai/safety-evals-for-gen-ai.pdf)) | David Blackwell | [Jonathan Bright](https://www.turing.ac.uk/people/researchers/jonathan-bright) |
| [13/11/23](#131123) | Technical: Introduction to Diffusion models ([Slides](https://github.com/alan-turing-institute/foundation-models-reading-group/blob/main/sessions/16-diffusion-models/intro_to_diffusion_models.pdf)) | David Blackwell | [Edmund Dable-Heath](https://github.com/eddableheath) |
| [20/11/23](#201123) | Research at Turing: Transformers for coding/software engineering ([Slides](https://github.com/alan-turing-institute/foundation-models-reading-group/blob/main/sessions/17-trasformers-for-coding/transformers-for-software-engineering-and-earlybird.pdf)) | Mae Jemison | [Anastasiia Grishina](https://www.turing.ac.uk/people/enrichment-students/anastasiia-grishina) |
| [04/12/23](#041223) | Discussion: Best Practice for Responsible Foundation Models ‚Äì What Should Developers Do and How You Can Help ([Slides](https://github.com/alan-turing-institute/foundation-models-reading-group/blob/main/sessions/18-best-practice-for-responsible-foundation-models/Best-Practice-for-Responsible-Foundation-Models.pdf)) | Ursula Franklin | [Carolyn Ashurst](https://www.turing.ac.uk/people/turing-research-fellows/carolyn-ashurst) |
| [11/12/23](#111223) | Technical: Stable Diffusion  ([Slides](https://github.com/alan-turing-institute/foundation-models-reading-group/tree/main/sessions/19-stable-diffusion/stable_diffusion.pdf)) | David Blackwell | [Edmund Dable-Heath](https://github.com/eddableheath) |
| [08/01/24](#080124) | Discussion: Benchmarking AI applications on GPUs ([Slides](https://github.com/alan-turing-institute/foundation-models-reading-group/blob/main/sessions/20-benchmarking-gpus/transformers-rcp-benchmarking.pdf)) | David Blackwell | [Tomas Lazauskas](https://github.com/tomaslaz), [David Llewellyn-Jones](https://github.com/llewelld) |
| [15/01/24](#150124) | Technical: Retentive Networks ([Slides](https://github.com/alan-turing-institute/foundation-models-reading-group/tree/main/sessions/21-retentive-networks/RetentiveNetworks.pdf)) | David Blackwell | [Ed Gunn](https://github.com/egunn-turing) |
| [22/01/24](#220124) | Research at Turing: Spatial Graph Patterning of Filamentous Structures | David Blackwell | [Kristina Ulicna](https://www.turing.ac.uk/people/research-associates/kristina-ulicna)|
| [29/01/24](#290124) | Technical: Vision Transformers Need Registers ([Slides](https://github.com/alan-turing-institute/foundation-models-reading-group/blob/main/sessions/23-vision-transformers-need-registers/vision-transformers-need-registers.pdf)) | David Blackwell | [Tom Davies](tomogwen) |
| [05/02/24](#050224) | Discussion: Existential Risk of AI? ([Slides](https://github.com/alan-turing-institute/foundation-models-reading-group/blob/main/sessions/24-ai-existential-risk/2024-02-05-Bokeria-AI-x-risk.pdf)) | David Blackwell | [Levan Bokeria](https://www.turing.ac.uk/people/research-engineering/levan-bokeria) |
| [12/02/24](#120224) | Technical: Mechanistic interpretability ([Slides](https://github.com/alan-turing-institute/foundation-models-reading-group/blob/main/sessions/25-mechanistic-interpretability/Mech%20Interp%20Intro%2C%20FM%20Reading%20Group.pdf)) | David Blackwell | [Praveen Selvaraj](https://github.com/pravsels) |
| [19/02/24](#190224) | Research at Turing: Longitudinal NLP ([Slides](https://github.com/alan-turing-institute/foundation-models-reading-group/blob/main/sessions/26-longitudinal-nlp/Dynamic%20Language%20Modelling_190224_reduced.pdf)) | David Blackwell | [Jenny Chim](https://j-chim.github.io/), [Talia Tseriotou](https://github.com/ttseriotou) |
| [26/02/24](#260224) | Research at Turing: Machine translation quality estimation ([Slides](https://github.com/alan-turing-institute/foundation-models-reading-group/blob/main/sessions/27-machine-translation-quality-estimation/MTQE_2024-02-26_Foundation_Models_Reading_Group.pdf)) | David Blackwell | [Radka Jersakova](https://www.turing.ac.uk/people/researchers/radka-jersakova), [Jo Knight](https://www.turing.ac.uk/people/researchers/joanna-knight) |
| [04/03/24](#040324) | Discussion: Expanding participatory governance for LLMs: case studies from BigCode, Aya Initiative, and Collective Intelligence Project ([Slides](https://github.com/alan-turing-institute/foundation-models-reading-group/blob/main/sessions/28-participatory-ai-governance/2024_ParticipatoryAIGovernance_JD.pdf)) | David Blackwell | [Jennifer Ding](https://www.turing.ac.uk/people/business-team/jennifer-ding) |
| [11/03/24](#110324) | Research at Turing: Applying Vision Transformers in Neuroscience ([Slides](https://github.com/alan-turing-institute/foundation-models-reading-group/blob/main/sessions/29-applying-vision-transformers-in-neuroscience/2024-03-11-foundation-model-of-the-mouse-visual-cortex.pdf)) | David Blackwell | [Bryan Li](https://bryanli.io/) |
| [18/03/24](#180324) | Research at Turing: Not even a Chinese Room: evaluating LLMs on code simulation | David Blackwell | [Emanuele La Malfa](https://www.cs.ox.ac.uk/people/emanuele.lamalfa/) |
| [08/04/24](#080424) | Technical: Paper overviews ([Slides](https://image-editing.notion.site/Representation-Engineering-a451cb7f5af048b6878bc96396dac5a1), [Slides](https://github.com/alan-turing-institute/foundation-models-reading-group/blob/main/sessions/31-quick-paper-overviews/Quick_overview_ Binoculars_Paper.pdf)) | Ursula Franklin | [Fede Nanni](https://github.com/fedenanni), [Markus Hauru](https://github.com/mhauru/), [Praveen Selvaraj](https://github.com/pravsels) |
| [15/04/24](#150424) | Research at Turing: Natural Logic-based Fact Verification with LLMs | David Blackwell | [Marek Strong](https://marekstrong.github.io/)  |
| [22/04/24](#220424) | Research at Turing: Learn how to learn and distil during learning - Using meta-learning and second order optimisation to prune the model | David Blackwell | [Yilei Liang](https://www.cst.cam.ac.uk/people/yl841) |
| [29/04/24](#290424) | Invited Talk: How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions ([Slides](https://github.com/alan-turing-institute/foundation-models-reading-group/blob/main/sessions/34-how-to-catch-ai-liar/LLM_Lie_detection.pdf)) | David Blackwell | [Lorenzo Pacchiardi](http://www.lorenzopacchiardi.me/) |
| [13/05/24](#130524) | Technical: Overview of LLM Security ([Slides](https://github.com/alan-turing-institute/foundation-models-reading-group/blob/main/sessions/35-llm-security/LLM_Security_slides.pdf)) | David Blackwell | [Ed Chapman](https://www.turing.ac.uk/people/research-engineering/edward-chapman), [Burak Hasircioglu](https://www.turing.ac.uk/people/researchers/burak-hasircioglu), [Ezzeldin Zaki](https://www.kth.se/profile/eshereen) |

# Material for sessions

## 20/03/23
### Introduction to Word Embeddings and Language modelling

**Main**
- [Don't Count, Predict! paper](https://aclanthology.org/P14-1023.pdf)
- [Word Embeddings (1)](https://www.ruder.io/word-embeddings-1/)
- [Word Embeddings (2)](https://www.ruder.io/word-embeddings-softmax/)
- [Word Embeddings (3)](https://www.ruder.io/secret-word2vec/)
- [Brief History of NLP (part 1)](https://medium.com/@antoine.louis/a-brief-history-of-natural-language-processing-part-1-ffbcb937ebce)
- [Brief History of NLP (part 2)](https://medium.com/@antoine.louis/a-brief-history-of-natural-language-processing-part-2-f5e575e8e37)

**Extra**
- [Deep Learning, NLP and Representations](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)
- [Stanford NLP with Deep Learning | Lecture 1: Intro & Word Vectors](https://youtu.be/rmVRLeJRkl4)
- [Speech and Language Processing - Chapter 6: Vector Semantics and Embeddings](https://web.stanford.edu/~jurafsky/slp3/6.pdf)
- [Stanford Large Language Models | Lecture 1: Introduction](https://stanford-cs324.github.io/winter2022/lectures/introduction/)
    
## 03/04/23
### Deep Learning Basics

**Main**
- [Neural Networks and Deep Learning | Chapter 1: Using neural nets to recognize handwritten digits](http://neuralnetworksanddeeplearning.com/chap1.html)
- [Neural Networks and Deep Learning | Chapter 2: How the backpropagation algorithm works](http://neuralnetworksanddeeplearning.com/chap2.html)
  - Alternatively, come along to [Phil and Jack's Lunchtime Tech Talk](https://github.com/alan-turing-institute/DataScienceSkills/wiki/Lunchtime-Tech-Talks) on Back Propagation (April 11th) - message on the `#hut23-robots-in-disguise` slack if you want to get a calendar invite for the session

**Extra**
- [Learning Deep Learning | Chapters 1 and 2](https://jack89roberts.github.io/learning-deep-learning/index.html)
- [Neural Networks by Hand | Feedforward Neural Networks](https://philswatton.github.io/neural-networks-by-hand/feedforward-neural-network.html)
- [Andrej Karparthy (YouTube): The spelled-out intro to neural networks and backpropagation](https://youtu.be/VMj-3S1tku0)

## 17/04/23
### Sequence-to-sequence models part I: RNNs/LSTMs

**Main**
- [Speech and Language Processing | Chapter 9: RNNs and LSTMs](https://web.stanford.edu/~jurafsky/slp3/9.pdf)
  - Read up to Section 9.5 (read pages 1-13)

**Extra**
- [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
- [NLP with Deep Learning Stanford Course | Lecture 5](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes05-LM_RNN.pdf)
  - Read up to Section 3 (read pages 1-11)
- [Stanford NLP with Deep Learning | Lecture 5: RNNs)](https://you.be/PLryWeHPcBs)

## 03/05/23
### Sequence-to-sequence models part II: Encoder-decoder models

**Main**
- [Speech and Language Processing | Chapter 9: RNNs and LSTMs](https://web.stanford.edu/~jurafsky/slp3/9.pdf)
  - Read from 9.5 to 9.8 (read pages 14-21)
  
**Extra**
- [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- [NLP with Deep Learning Stanford Course | Lecture 5](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes05-LM_RNN.pdf)
  - Read from Section 3 (read pages 11 onwards)
- [Stanford NLP with Deep Learning | Lecture 6: Simple LSTM RNNs](https://youtu.be/0LixFSa7yts)

## 31/05/23
### Attention

**Main**
- [Attention is all you need](https://arxiv.org/abs/1706.03762)

**Extra**
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [Andrej Karpathy's GPT-2 from scratch](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=4766s)
- [Anthropic's Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html)
- [NLP with Deep Learning Stanford Course | Lecture 6](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes06-NMT_seq2seq_attention.pdf)
- [Attention and Augmented Recurrent Neural Networks](https://distill.pub/2016/augmented-rnns/)
- [Stanford NLP with Deep Learning | Lecture 7: Translation, Seq2Seq, Attention](https://youtu.be/wzfWHP6SXxY)
- [Stanford NLP with Deep Learning | Lecture 9: Self- Attention and Transformers](https://youtu.be/ptuGllU5SQQ)
- [Michigan Deep Learning for Comp Vis | Lecture 13: Attention](https://www.youtube.com/watch?v=YAgjfMR9R_M)

## 26/06/23
### Transformer Encoder and Decoders

**Main**
- [Attention is all you need](https://arxiv.org/abs/1706.03762)
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [The Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/)

**Extra**
- [Speech and Language Processing | Chapter 9: RNNs and LSTMs](https://web.stanford.edu/~jurafsky/slp3/9.pdf)
  - Read from 9.8 (read pages from 21 onwards)
- [Speech and Language Processing | Chapter 10: Transformers and Pretrained Language Models](https://web.stanford.edu/~jurafsky/slp3/10.pdf)
- [NLP with Deep Learning Stanford Course | Self-Attention & Transformers](https://web.stanford.edu/class/cs224n/readings/cs224n-self-attention-transformers-2023_draft.pdf)

## 10/07/23
### BERT: Masked Language modelling and Pre-training

**Main**
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)

**Extra**
- [Speech and Language Processing | Chapter 11: Fine-Tuning and Masked Language Models](https://web.stanford.edu/~jurafsky/slp3/11.pdf)
- [The Illustrated BERT](http://jalammar.github.io/illustrated-bert/)
- [BERT 101 ü§ó State Of The Art NLP Model Explained](https://huggingface.co/blog/bert-101)
- [Paper summary ‚Äî BERT](https://medium.com/analytics-vidhya/paper-summary-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-861456fed1f9)

## 24/07/23
### GPT: Pretraining Decoders

**Main**
- [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)

**Extra**
- [Paper summary - Improving Language Understanding by Generative Pre-Training](https://sh-tsang.medium.com/review-gpt-improving-language-understanding-by-generative-pre-training-28f30d39cd10)
- [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)

**Note** the below materials for other sessions, or are not confirmed

## 07/08/23
### Vision Transformers part I

**Main**
- [Gradient Based Learning Applied to Document Recognition](https://ieeexplore.ieee.org/document/726791)
- [An Image Is Worth 16x16 Words: Transformers for Image Recongition at scale](https://arxiv.org/pdf/2010.11929.pdf)

**Extra**
- [Vision Transformer for Image Classification - Shusen Wang (YouTube)](https://youtu.be/HZ4j_U3FC94)
- [Neocognitron: A Self-organizing Neural Network Model
for a Mechanism of Pattern Recognition
Unaffected by Shift in Position](https://www.rctn.org/bruno/public/papers/Fukushima1980.pdf)
- [But what is a convolution? - 3Blue1Brown (YouTube)](https://www.youtube.com/watch?v=KuXjwB4LzSA&t=705s)
- [CNN Explainer](https://poloclub.github.io/cnn-explainer/)

## 21/08/23
### Vision Transformers part II

**Main**
- [Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers](https://arxiv.org/abs/2012.15840)

**Extra**
- [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872)

## 18/09/23
### LoRA (+ parameter efficient fine-tuning)

**Main**
- [Huggingface PEFT library](https://huggingface.co/docs/peft/index)

**Extra**
- [LoRA: Low-Rank Adaption of Large Language Models](https://arxiv.org/pdf/2106.09685.pdf)
- [LoRA conceptual guide](https://huggingface.co/docs/peft/conceptual_guides/lora)

## 02/10/23
### Reinforcement Learning Human Feedback (RLHF)

**Main**
- [RLHF: Reinforcement Learning from Human Feedback](https://huyenchip.com/2023/05/02/rlhf.html)
- [Illustrating Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf)

**Extra**
- [An introduction to Reinforcement Learning](https://youtu.be/JgvyzIkgxF0)
- [Understanding Reinforcement Learning from Human Feedback (RLHF): Part 1](https://wandb.ai/ayush-thakur/RLHF/reports/Understanding-Reinforcement-Learning-from-Human-Feedback-RLHF-Part-1--VmlldzoyODk5MTIx)
- [Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2307.15217)
- [Secrets of RLHF in Large Language Models Part I: PPO](https://arxiv.org/abs/2307.04964)

**Beyond RLHF**
- [Reinforced Self-Training (ReST) for Language Modeling](https://arxiv.org/abs/2308.08998)
- [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290)

## 16/10/23
### Prompt Engineering

**Guides**
- [Anthropic](https://docs.anthropic.com/claude/docs/introduction-to-prompt-design)
- [AssemblyAI](https://www.assemblyai.com/docs/guides/lemur-best-practices)

**Videos**
- [Jeremy Howard](https://www.youtube.com/watch?v=jkrNMKz9pWU&t=1475s)
- [Andrej Karpathy](https://www.youtube.com/watch?v=bZQun8Y4L2A&t=408s)

**Meta**
- [Awesome-Prompt-Engineering](https://github.com/promptslab/Awesome-Prompt-Engineering)

## 30/10/23
### Knowledge retrieval FMs

**Main**
- [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)

**Extra**
- [Retrieval Augmented Generation: Streamlining the creation of intelligent natural language processing models](https://ai.meta.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/)
- [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/pdf/2004.04906.pdf)
- [Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/pdf/2307.03172.pdf)
- [Self-RAG: Learning to Retrieve, Generate And Critique Through Self-Reflection](https://arxiv.org/pdf/2310.11511.pdf)

## 06/11/23
### Current challenges and future directions in safety evaluations for generative AI

**Main**
- [Sociotechnical Safety Evaluation of Generative AI Systems](https://arxiv.org/pdf/2310.11986.pdf)

**Extras**
- [Challenges in evaluating AI systems - Anthropic](https://www.anthropic.com/index/evaluating-ai-systems)
- [AI and Catastrophic Risk - Y. Bengio](https://www.journalofdemocracy.org/ai-and-catastrophic-risk/)

## 13/11/23
### Technical: Introduction to Diffusion models

There are plenty of blog-posts and top level overviews of diffusion models which explain the main idea of, 'running a noisy blurring process backwards from the noise', however for more technical reading (which I will warn are quite heavy on the maths) the main two papers are:
- [Denoising Diffusion Probabilistic Models](https://arxiv.org/pdf/2006.11239.pdf)
- [DENOISING DIFFUSION IMPLICIT MODELS](https://arxiv.org/pdf/2010.02502.pdf)

Both are about the sampling methods used in the process (notably without the inclusion of context that allows for text-to-image generation). For a general overview the following is fairly good:
- [Diffusion Models: A Comprehensive Survey of Methods and Applications](https://arxiv.org/pdf/2209.00796.pdf)

And if you're curious (and want spoilers) about stable diffusion and latent diffusion models [this is the main paper](https://arxiv.org/pdf/2112.10752.pdf).

## 20/11/23
### Transformers for coding/software engineering

**Main**
- [The EarlyBIRD Catches the Bug: On Exploiting Early Layers of Encoder Models for More Efficient Code Classification](https://arxiv.org/abs/2305.04940)

**Extra**
- [Large Language Models for Software Engineering: Survey and Open Problems](https://arxiv.org/abs/2310.03533)
- [A Systematic Evaluation of Large Language Models of Code](https://arxiv.org/abs/2202.13169)
- [Automated Program Repair in the Era of Large Pre-trained Language Models](https://lingming.cs.illinois.edu/publications/icse2023a.pdf)
- [A Survey on Language Models for Code](https://arxiv.org/pdf/2311.07989v1.pdf)

## 04/12/23
### Guidance for Safe Foundation Model Deployment

**Main**
- [PAI‚Äôs Guidance for Safe Foundation Model Deployment](https://partnershiponai.org/modeldeployment/)

## 11/12/23
### Stable Diffusion

**Main**
- [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/pdf/2112.10752.pdf).

## 08/01/24
### Benchmarking AI applications on GPUs

**Main**
- [Benchmarking AI applications on GPUs Slides PDF](https://github.com/alan-turing-institute/foundation-models-reading-group/blob/main/sessions/20-benchmarking-gpus/transformers-rcp-benchmarking.pdf)

**Extra**
- [Repo containing write-up and LaTeX source for the slides](https://github.com/alan-turing-institute/rc-gpt2-performance-benchmarking-workspace) (may not be available immediately)
- [lightning-GPT](https://github.com/Lightning-Universe/lightning-GPT)
- [Brendan Bycroft's LLM Viz](https://bbycroft.net/llm)
- [Hacked visualisaion for illustrating GPT-2 models](https://github.com/llewelld/llm-viz/tree/gpt2)

## 15/01/24
### Retentive Networks

**Main**
- [Retentive Network: A Successor to Transformer for Large Language Models](https://arxiv.org/abs/2307.08621)

## 22/01/24
### Spatial Graph Patterning of Filamentous Structures

**Main**
- [GRACE - Graph Representation Analysis for Connected Embeddings](https://github.com/alan-turing-institute/grace)

## 29/01/24
### Vision Transformers Need Registers

**Main**
- [Vision Transformers Need Registers](https://arxiv.org/abs/2309.16588)

**Extra**
- [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)

## 05/02/24
### Existential Risk of AI?

**Main**
- [Preventing an AI-related catastrophe](https://80000hours.org/problem-profiles/artificial-intelligence/)

**Extra**
- [Podcast: The 80,000 Hours Podcast on Artificial Intelligence](https://80000hours.org/podcast/on-artificial-intelligence/)
- Book: Nick Bostrom - "Superintelligence"
- Book: Toby Ord - "The Precipice"

## 12/02/24
### Mechanistic interpretability

**Main** 
- [(2020) Zoom In: An Introduction to Circuits](https://distill.pub/2020/circuits/zoom-in/)
- [(2021) A Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html)

**Supplementary**
- [CVPR tutorial: Intro to Circuits in CNNs by Chris Olah](https://www.youtube.com/watch?v=gXsKyZ_Y_i8)
- [Transformer Circuits Playlist](https://www.youtube.com/watch?v=V3NQaDR3xI4&list=PLoyGOS2WIonajhAVqKUgEMNmeq3nEeM51)
- [Neel Nanda's Walkthrough: A Mathematical Framework for Transformer Circuits](https://www.youtube.com/watch?v=KV5gbOmHbjU)

**Extras**
- [Build GPT2 from scratch - Neel Nanda](https://www.youtube.com/watch?v=bOYE6E8JrtU&t=2s)
- [Mechinterp vs Neuroscience - Chris Olah](https://colah.github.io/notes/interp-v-neuro/)

## 19/02/24
### Longitudinal NLP

**Main**
- [Combining Hierachical VAEs with LLMs for Clinically Meaningful Timeline Summarisation in Social Media](https://arxiv.org/pdf/2401.16240.pdf)
- [Sig-Networks Toolkit: Signature Networks for Longitudinal Language Modelling](https://arxiv.org/pdf/2312.03523.pdf)

**Extra**
- [Identifying Moments of Change from Longitudinal User Text](https://aclanthology.org/2022.acl-long.318.pdf)
- [ROFORMER: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/pdf/2104.09864.pdf)
- [Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?](https://arxiv.org/pdf/2202.12837.pdf)

## 26/02/24
### Machine Translation Quality Estimation

## 04/03/24
### Expanding participatory governance for LLMs: case studies from BigCode, Aya Initiative, and Collective Intelligence Project

**Main**
- [Towards Openness Beyond Open Access: User Journeys through 3 Open AI Collaboratives](https://arxiv.org/abs/2301.08488)
- [The BigCode Project Governance Card](https://arxiv.org/abs/2312.03872)
- Aya Initiative: [website](https://txt.cohere.com/aya-multilingual/), [data paper](https://cohere.com/research/papers/aya-dataset-paper-2024-02-13)
- [Collective Intelligence Project white paper](https://cip.org/whitepaper)

## 11/03/24
### Applying Vision Transformers in Neuroscience

**Main**
- [2022 in review: neuroAI comes of age](https://xcorr.net/2023/01/01/2022-in-review-neuroai-comes-of-age/)
- [Towards a Foundation Model of the Mouse Visual Cortex](https://www.biorxiv.org/content/10.1101/2023.03.21.533548v2)
- [V1T: large-scale mouse V1 response prediction using a Vision Transformer](https://openreview.net/forum?id=qHZs2p4ZD4)

## 18/03/24
### Not even a Chinese Room: evaluating LLMs on code simulation

**Main**
- [CodeMind: A Framework to Challenge Large Language Models for Code Reasoning](https://arxiv.org/pdf/2402.09664.pdf)
- [Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap](https://arxiv.org/pdf/2402.19450.pdf)
- [The Counterfeit Conundrum: Can Code Language Models Grasp the Nuances of Their Incorrect Generations?](https://arxiv.org/pdf/2402.19475.pdf)
- [THE GENERATIVE AI PARADOX: ‚ÄúWhat It Can Create, It May Not Understand‚Äù](https://openreview.net/pdf?id=CF8H8MS5P8)

## 08/04/24
### Paper overviews

**Main**
- [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/pdf/2402.17764)
- [AI Transparancy Technique](https://www.ai-transparency.org/)
- [Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated Text](https://arxiv.org/abs/2401.12070)
- [Binoculars GitHub repo](https://github.com/ahans30/Binoculars?tab=readme-ov-file)

## 15/04/24
### Natural Logic-based Fact Verification with LLMs

## 22/04/24
### Learn how to learn and distil during learning - Using meta-learning and second order optimisation to prune the model

**Main**
- [Fisher-Legendre (FishLeg) optimization of deep neural networks](https://openreview.net/pdf?id=c9lAOPvQHS)
- [Second order derivatives for network pruning: Optimal Brain Surgeon](https://proceedings.neurips.cc/paper/1992/file/303ed4c69846ab36c2904d3ba8573050-Paper.pdf)
- [The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models](https://arxiv.org/pdf/2203.07259.pdf)

## 29/04/24
### How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions

**Main**
- [How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions](https://arxiv.org/abs/2309.15840)

## 13/05/24
### Overview of LLM Security

**Main**
- [HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal](https://arxiv.org/pdf/2402.04249)
- [FINE-TUNING ALIGNED LANGUAGE MODELS COMPROMISES SAFETY, EVEN WHEN USERS DO NOT INTEND TO!](https://arxiv.org/pdf/2310.03693)

## Miscellaneous

### Tokenizers and Huggingface tutorial

**Main**
- [Huggingface Tokenizer tutorial](https://huggingface.co/learn/nlp-course/chapter2/4?fw=pt)
- [Huggingface `transformers` course](https://huggingface.co/learn/nlp-course/chapter2/1?fw=pt)
